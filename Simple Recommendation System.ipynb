{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTnGjmPVXgvy"
   },
   "source": [
    "In this assignment we design a recomendation engine (*Don't worry about the effectiveness of the system. It maybe very bad. The idea is just to offer you a proof of concept!*). The recommendation engine suggests the students a module that closely matches the modules already taken by the student. The dataset comprices of two files:\n",
    "- List of modules in the School of Computing \n",
    "- List of graduated students and the modules they had taken during their studies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgL9CZ-pXgvz"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Tjl54HB5Xgvz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "'''\n",
    "    YOU MUST USE THE RANDOM SEED WHEREVER NEEDED OR RANDOM_STATE as 42.\n",
    "'''\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "courses = pd.read_csv(\"courses.tsv\", sep='\\t')\n",
    "students = pd.read_csv(\"students.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "      <th>credits</th>\n",
       "      <th>workload</th>\n",
       "      <th>info</th>\n",
       "      <th>specialisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS1010</td>\n",
       "      <td>Programming Methodology</td>\n",
       "      <td>4</td>\n",
       "      <td>2-1-1-3-3</td>\n",
       "      <td>This module introduces the fundamental concept...</td>\n",
       "      <td>Core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS1010FC/X</td>\n",
       "      <td>Programming Methodology</td>\n",
       "      <td>4</td>\n",
       "      <td>2-1-1-3-3</td>\n",
       "      <td>This module introduces the fundamental concept...</td>\n",
       "      <td>Core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS1010E</td>\n",
       "      <td>Programming Methodology</td>\n",
       "      <td>4</td>\n",
       "      <td>2-1-1-3-3</td>\n",
       "      <td>This module introduces the fundamental concept...</td>\n",
       "      <td>Core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS1010J</td>\n",
       "      <td>Programming Methodology</td>\n",
       "      <td>4</td>\n",
       "      <td>2-1-1-3-3</td>\n",
       "      <td>This module introduces the fundamental concept...</td>\n",
       "      <td>Core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS1010S</td>\n",
       "      <td>Programming Methodology</td>\n",
       "      <td>4</td>\n",
       "      <td>2-1-1-3-3</td>\n",
       "      <td>This module introduces the fundamental concept...</td>\n",
       "      <td>Core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>CS6281</td>\n",
       "      <td>Topics in Computer Science II</td>\n",
       "      <td>4</td>\n",
       "      <td>3-0-0-3-4</td>\n",
       "      <td>Topics will be of an advanced computer science...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>CS6282</td>\n",
       "      <td>Topics in Computer Science III</td>\n",
       "      <td>4</td>\n",
       "      <td>3-0-0-3-4</td>\n",
       "      <td>Topics will be of an advanced computer science...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>CS6283</td>\n",
       "      <td>Topics in Computer Science IV</td>\n",
       "      <td>4</td>\n",
       "      <td>3-0-0-3-4</td>\n",
       "      <td>Topics will be of an advanced computer science...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>CS6284</td>\n",
       "      <td>Topics in Computer Science V</td>\n",
       "      <td>4</td>\n",
       "      <td>3-0-0-3-4</td>\n",
       "      <td>Topics will be of an advanced computer science...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>CS6285</td>\n",
       "      <td>Topics in Computer Science VI</td>\n",
       "      <td>4</td>\n",
       "      <td>3-0-0-3-4</td>\n",
       "      <td>Topics will be of an advanced computer science...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           code                            name  credits    workload  \\\n",
       "0        CS1010         Programming Methodology        4   2-1-1-3-3   \n",
       "1    CS1010FC/X         Programming Methodology        4   2-1-1-3-3   \n",
       "2       CS1010E         Programming Methodology        4   2-1-1-3-3   \n",
       "3       CS1010J         Programming Methodology        4   2-1-1-3-3   \n",
       "4       CS1010S         Programming Methodology        4   2-1-1-3-3   \n",
       "..          ...                             ...      ...         ...   \n",
       "179      CS6281   Topics in Computer Science II        4   3-0-0-3-4   \n",
       "180      CS6282  Topics in Computer Science III        4   3-0-0-3-4   \n",
       "181      CS6283   Topics in Computer Science IV        4   3-0-0-3-4   \n",
       "182      CS6284    Topics in Computer Science V        4   3-0-0-3-4   \n",
       "183      CS6285   Topics in Computer Science VI        4   3-0-0-3-4   \n",
       "\n",
       "                                                  info specialisation  \n",
       "0    This module introduces the fundamental concept...           Core  \n",
       "1    This module introduces the fundamental concept...           Core  \n",
       "2    This module introduces the fundamental concept...           Core  \n",
       "3    This module introduces the fundamental concept...           Core  \n",
       "4    This module introduces the fundamental concept...           Core  \n",
       "..                                                 ...            ...  \n",
       "179  Topics will be of an advanced computer science...            NaN  \n",
       "180  Topics will be of an advanced computer science...            NaN  \n",
       "181  Topics will be of an advanced computer science...            NaN  \n",
       "182  Topics will be of an advanced computer science...            NaN  \n",
       "183  Topics will be of an advanced computer science...            NaN  \n",
       "\n",
       "[184 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKrejjTxXgv0"
   },
   "source": [
    "# Question 1: Creating the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ_EU5o9Xgv1"
   },
   "source": [
    "We want to create a sklearn pipeline to efficiently preprocess the data and prepare it for training a model. We use three different features in the `courses` data: `specialisation`, `info` and `workload`. We want to represent every feature in a numeric form and merge them to form a feature vector for every course. We do so in the following way:\n",
    "- `specialisation` represents one of the six levels of the module. For instance: CS2103 is a Software Engineering (SE) specialisation module. Encode this categorical feature into a vector. The decision of handling missing values is left to you! *(Hint: You can use `MultiLabelBinerizer` to do so.)*\n",
    "- `info` provides a short discription of the module. We want to convert it into a vector using CountVectorizer. *Don't forget to remove the stopwords* while doing so.\n",
    "-  `workload` states the intended distribution of workload over lectures, tutorials, labs and self study. We want to find the workload as the sum of individual workloads. For instnce: 3-1-1-3-2 workload transforms to 10 hours.\n",
    "\n",
    "Provide implementation for three classes that help us build the pipeline. `transformed_courses` should be a numpy array of shape `[n_courses X n_features]`.\n",
    "\n",
    "                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sVBlKTN6Xgv1"
   },
   "outputs": [],
   "source": [
    "class WorkloadTransformer:        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        transformed = X['workload'].str.split('-').apply(lambda x: sum(float(val) for val in x))\n",
    "        return transformed.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EZ_fhJkFXgv2"
   },
   "outputs": [],
   "source": [
    "class InfoTransformer:   \n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        self.vectorizer.fit(X['info'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        transformed = self.vectorizer.transform(X['info'])\n",
    "        return transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oLKGAPR1Xgv3"
   },
   "outputs": [],
   "source": [
    "class SpecTransformer: \n",
    "    def __init__(self):\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        self.mlb.fit(X['specialisation'].fillna('Missing').values.reshape(-1, 1))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        transformed = self.mlb.transform(X['specialisation'].fillna('Missing').values.reshape(-1, 1))\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kcx2rx6CXgv4"
   },
   "outputs": [],
   "source": [
    "featureTransformer = FeatureUnion([\n",
    "    ('workload_processing', Pipeline([('wrkld', WorkloadTransformer())])),\n",
    "    ('info_processing', Pipeline([('info', InfoTransformer())])),\n",
    "    ('spec_processing', Pipeline([('spec', SpecTransformer())])),\n",
    "])\n",
    "\n",
    "featureTransformer.fit(courses)\n",
    "transformed_courses = featureTransformer.transform(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 2139)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_courses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtc4ObIlXgv5"
   },
   "source": [
    "Now we prepare our testing data in the same way we preprocessed the course. `students` data comprises of 1000 students and a list of modules they have taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQu6wIgYXgv6"
   },
   "source": [
    "Create `Xtest` and `Ytest` as two matrices. `Xtest`, of size `1000*5`, comprises of first five modules for every student in the list. `Ytest`, of size `1000*[remaining_modules]`, comprises of rest of the modules for every student in the list. \n",
    "We do so in order to assess the performance of the recommender. We assess the recommender based on its effectiveness to predict the modules given a list of five modules as the input.\n",
    "\n",
    "For instance: \n",
    "- `Xtest[0] = ['CS2105', 'CS4222', 'CS6270', 'CS6205', 'CS4226']`\n",
    "- `Ytest[0] = ['CS3282', 'CS6204', 'CS5223', 'CS3281', 'CS4344', 'CS5422', 'CS3237', 'CS5233']`.\n",
    "\n",
    "<div align=\"right\"></align>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "students['courses'] = students['courses'].str.split(',')\n",
    "Xtest = students['courses'].apply(lambda x: x[:5]).tolist()\n",
    "Ytest = students['courses'].apply(lambda x: x[5:]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-D7LhvXgv7"
   },
   "source": [
    "For every student in `Xtest`, we need to transform the list of 5 modules to the feature space using the `featureTransformer` fit on the training data. For every module we will get a feature vector of size `n_features`. We *add* these feature vectors to get an aggregate feature vector for very student.\n",
    "\n",
    "Write a function `getFeatureVector` that takes in the list of modules and `featureTransformer`. It returns the feature vector for the specified list of courses. For instance, `getFeatureVector(Xtest[0], featureTransformer)` will return a vector of size `n_features`.\n",
    "\n",
    "<div align=\"right\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVector(modules, featureTransformer):\n",
    "    filtered_courses = courses[courses['code'].isin(modules)]\n",
    "    transformed_modules = featureTransformer.transform(filtered_courses)\n",
    "    aggregate_feature_vector = np.sum(transformed_modules, axis=0)\n",
    "    return aggregate_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2139,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFeatureVector(Xtest[0], featureTransformer).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuwxpWB2Xgv7"
   },
   "source": [
    "# Question 2: Content based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJbpI-xdXgv8"
   },
   "source": [
    "We can use a model as simple as K-nearest neighbour (KNN) to perform a content based recommendation. If we provide a list of 5 modules to the recommender, it provide us a list of modules that are similar to the specified modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQEMNzrcXgv8"
   },
   "source": [
    "`sklearn` provides `NearestNeighbors` as well as `KNeighborsClassifier`, both of which have a similar functionality. `NearestNeighbors` provides as an easy functionality to predict a list of K nearest neighbours. Therefore, we prefer it over `KNeighborsClassifier`. If we want to find K nearest points to a datapoint`d`, we need to use `n_neighbors` as K + 1 because the list includes `d` itself.\n",
    "\n",
    "You can now train the model using the training data, which comprises of `transformed_courses` and with their codes as the labels. \n",
    "<div align=\"right\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ezqrA0XsXgv8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=6)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', n_neighbors=6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "model = NearestNeighbors(algorithm = \"brute\", n_neighbors = K + 1)\n",
    "model.fit(transformed_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Oo47yaXgv9"
   },
   "source": [
    "It is time to see our model in action. Let's see what modules our model reommends based on the modules taken by a student.\n",
    "\n",
    "Write a function that takes in a *pre-trained* model of your choice as input and the list of modules. It returns the top-K recommendations of the model. Print the top 6 recommendations for the first student. \n",
    "<div align=\"right\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS3203', 'CS3205', 'CS5223', 'CS2020', 'CS3216', 'CS3217']\n"
     ]
    }
   ],
   "source": [
    "def recommend(model, modulesTaken, k=5):\n",
    "    course_codes = courses['code'].tolist()\n",
    "    feature_vector = getFeatureVector(modulesTaken, featureTransformer)\n",
    "    distances, indices = model.kneighbors([feature_vector],n_neighbors=k+1)\n",
    "    recommended_indices = indices[0]\n",
    "    recommended_courses = [course_codes[i] for i in recommended_indices]\n",
    "    recommended_courses = [course for course in recommended_courses]\n",
    "    return recommended_courses[:k]\n",
    "print(recommend(model, Xtest[0], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVhPPoPXgv9"
   },
   "source": [
    "# Question 3: Recommender evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrxDQQWCXgv-"
   },
   "source": [
    "Is this the model any good? To assess the performance of the model, we use **precision** and **recall** as our metrics. `Ytest` consists of true labels for every students. Using those labels as the ground truth, compute the precision and recall for every student. Write a code that prints values of average precision and recall for a specific value of `K` over the `students` dataset. Print the value of average precision and average recall for `K= 10`.\n",
    "\n",
    "                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IPyZswBBXgv-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.057\n",
      "Average Recall: 0.056304128942093336\n"
     ]
    }
   ],
   "source": [
    "def compute_precision_recall(model, Xtest, Ytest, k):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for modules_taken, true_labels in zip(Xtest, Ytest):\n",
    "        recommended = recommend(model, modules_taken, k)\n",
    "        relevant_recommendations = len(set(recommended).intersection(set(true_labels)))\n",
    "        precision = relevant_recommendations / len(recommended) if recommended else 0\n",
    "        precisions.append(precision)\n",
    "        recall = relevant_recommendations / len(true_labels) if true_labels else 0\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "avg_precision, avg_recall = compute_precision_recall(model, Xtest, Ytest, 10)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeR2w7IpXgv-"
   },
   "source": [
    "We observe that both precision and recall is not really great. The reason might be igh feature dimension, which may even be noisy. Append the exisiting `featureTransformer` with a PCA to reduce the dimension. \n",
    "\n",
    "Print the value of average precision and recall for `K= 10` after the introduction of PCA.\n",
    "\n",
    "                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.22255\n",
      "Average Recall: 0.046578402084671434\n"
     ]
    }
   ],
   "source": [
    "pca_transformer = Pipeline([\n",
    "    ('features', featureTransformer),\n",
    "    ('pca', PCA(n_components=100))  \n",
    "])\n",
    "\n",
    "pca_transformer.fit(courses)\n",
    "\n",
    "transformed_courses_pca = pca_transformer.transform(courses)\n",
    "\n",
    "course_codes = courses['code'].tolist()\n",
    "\n",
    "model_pca = NearestNeighbors(algorithm=\"brute\", n_neighbors=K+1)\n",
    "model_pca.fit(transformed_courses_pca)\n",
    "\n",
    "def getFeatureVector(modules, featureTransformer):\n",
    "    filtered_courses = courses[courses['code'].isin(modules)]\n",
    "    if filtered_courses.empty:\n",
    "        return np.zeros(featureTransformer.named_steps['pca'].n_components_)\n",
    "    transformed_modules = featureTransformer.transform(filtered_courses)\n",
    "    aggregate_feature_vector = np.sum(transformed_modules, axis=0)\n",
    "    return aggregate_feature_vector\n",
    "\n",
    "def recommend(model, modulesTaken, k=5):\n",
    "    feature_vector = getFeatureVector(modulesTaken, pca_transformer)\n",
    "    distances, indices = model.kneighbors([feature_vector])\n",
    "    recommended_indices = indices[0]\n",
    "    recommended_courses = [course_codes[i] for i in recommended_indices]\n",
    "    recommended_courses = [course for course in recommended_courses if course not in modulesTaken]\n",
    "    return recommended_courses[:k]\n",
    "\n",
    "avg_precision_pca, avg_recall_pca = compute_precision_recall(model_pca, Xtest, Ytest, 10)\n",
    "print(\"Average Precision:\", avg_precision_pca)\n",
    "print(\"Average Recall:\", avg_recall_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above results, it can be seen that the model's precision reaches more than 21% with the addition of PCA, which is an improvement of about 16%, and the recall is also reduced by about 1%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih2GatUQXgv_"
   },
   "source": [
    "Can you provide some **concrete** (something that you can implement) suggestions to improve the performance of the system? The improvement does not have to be very significant.\n",
    "\n",
    "                                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some concrete steps we can take to potentially improve the performance of the recommendation system:\n",
    "\n",
    "**1. Enhance Text Preprocessing for 'info' Feature:**\n",
    "* Instead of just removing stopwords, we can also include stemming or lemmatization to process the text in the 'info' column. This might help in getting a better representation of the text.\n",
    "* Instead of using simple term frequencies from 'CountVectorizer' for the course descriptions, we can switch to the TF-IDF representation, which will give more weight to terms that are unique to a particular document and less weight to terms that are common across documents.\n",
    "* Adjust parameters of the 'TfidfVectorizer' like 'max_df', 'min_df', and 'ngram_range' to fine-tune the feature extraction from text.\n",
    "\n",
    "**2. Handle Missing Values in specialisation Feature:**\n",
    "* Instead of just filling missing values with 'Missing', we can fill them by 'SimpleImputer' with the mode (most frequent value) of the 'specialisation' column.\n",
    "\n",
    "**3. Feature Scaling:**\n",
    "* After obtaining the feature matrix from the 'featureTransformer', we can apply feature scaling (e.g., MinMaxScaler) to ensure that all features have the same scale. This is particularly important for KNN which is distance-based.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.24341666666666667\n",
      "Average Recall: 0.03865637150087615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class EnhancedInfoTransformer:   \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=2, ngram_range=(1, 2))\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.vectorizer.fit(X['info'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        transformed = self.vectorizer.transform(X['info'])\n",
    "        return transformed.toarray()\n",
    "\n",
    "class EnhancedSpecTransformer: \n",
    "    def __init__(self):\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.imputer = SimpleImputer(strategy='most_frequent')\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        filled_data = self.imputer.fit_transform(X['specialisation'].values.reshape(-1, 1))\n",
    "        self.mlb.fit(filled_data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        filled_data = self.imputer.transform(X['specialisation'].values.reshape(-1, 1))\n",
    "        transformed = self.mlb.transform(filled_data)\n",
    "        return transformed\n",
    "\n",
    "enhanced_featureTransformer1 = FeatureUnion([\n",
    "    ('workload_processing', Pipeline([('wrkld', WorkloadTransformer())])),\n",
    "    ('info_processing', Pipeline([('info', EnhancedInfoTransformer())])),\n",
    "    ('spec_processing', Pipeline([('spec', EnhancedSpecTransformer())])),\n",
    "])\n",
    "\n",
    "enhanced_featureTransformer = Pipeline([\n",
    "    ('features', enhanced_featureTransformer1),\n",
    "    ('pca', PCA(n_components=100))  \n",
    "])\n",
    "\n",
    "enhanced_featureTransformer.fit(courses)\n",
    "enhanced_transformed_courses = enhanced_featureTransformer.transform(courses)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_courses = scaler.fit_transform(enhanced_transformed_courses)\n",
    "\n",
    "model2 = NearestNeighbors(algorithm=\"brute\", n_neighbors=K+1)\n",
    "model2.fit(scaled_courses)\n",
    "\n",
    "def enhanced_getFeatureVector(modules, featureTransformer, scaler):\n",
    "    filtered_courses = courses[courses['code'].isin(modules)]\n",
    "    if filtered_courses.empty:\n",
    "        return np.zeros(featureTransformer.named_steps['pca'].n_components_)\n",
    "    transformed_modules = featureTransformer.transform(filtered_courses)\n",
    "    aggregate_feature_vector = np.sum(transformed_modules, axis=0)\n",
    "    \n",
    "    # Scale the aggregate feature vector\n",
    "    scaled_vector = scaler.transform([aggregate_feature_vector])\n",
    "    \n",
    "    return scaled_vector[0]\n",
    "\n",
    "def enhanced_recommend(model, modulesTaken, k=5):\n",
    "    course_codes = courses['code'].tolist()\n",
    "    feature_vector = enhanced_getFeatureVector(modulesTaken, enhanced_featureTransformer, scaler)\n",
    "    \n",
    "    distances, indices = model2.kneighbors([feature_vector])\n",
    "    recommended_indices = indices[0]\n",
    "    \n",
    "    # Filter the courses the student has taken and get the top k\n",
    "    recommended_courses = [course_codes[i] for i in recommended_indices if course_codes[i] not in modulesTaken][:k]\n",
    "    \n",
    "    return recommended_courses\n",
    "def compute_precision_recall(model, Xtest, Ytest, k):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for modules_taken, true_labels in zip(Xtest, Ytest):\n",
    "        recommended = enhanced_recommend(model2, modules_taken, k)\n",
    "        relevant_recommendations = len(set(recommended).intersection(set(true_labels)))\n",
    "        precision = relevant_recommendations / len(recommended) if recommended else 0\n",
    "        precisions.append(precision)\n",
    "        recall = relevant_recommendations / len(true_labels) if true_labels else 0\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "avg_precision_enhanced, avg_recall_enhanced = compute_precision_recall(model2, Xtest, Ytest, 10)\n",
    "print(\"Average Precision:\", avg_precision_enhanced)\n",
    "print(\"Average Recall:\", avg_recall_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above results, it can be seen that the model has been improved to achieve a precision of more than 23%, which is an improvement of about 2%-3%, and the recall has been reduced by about 1%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
